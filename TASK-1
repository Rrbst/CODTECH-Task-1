This code implements a Generative Adversarial Network (GAN) using the Fashion MNIST dataset, which consists of grayscale images of clothing items. Let's break it down step by step:

1. Data Loading and Preprocessing:
The dataset is provided in a zip file (fashion-mnist_test.csv.zip). The zip file is extracted, and the Fashion MNIST CSV file is loaded using pandas.
The labels are separated from the pixel data. The pixel data (X_train) is reshaped to have the shape (num_samples, 28, 28, 1) (i.e., 28x28 images with 1 color channel).
The pixel values are normalized to a range of [0, 1] by dividing by 255.0 (since original pixel values are from 0 to 255).
2. Building the Generator Model:
The Generator generates new images from random noise. It uses a fully connected neural network structure.
The layers in the generator include:
Dense layers followed by Leaky ReLU activations and Batch Normalization to stabilize training.
The final output layer reshapes the output to a 28x28x1 image with a tanh activation (output values range from -1 to 1).
Generator Architecture:

Input: A random noise vector (100-dimensional).
Hidden layers: 256, 512, and 1024 units, each followed by a Leaky ReLU activation and Batch Normalization.
Output layer: A single 28x28 image.
3. Building the Discriminator Model:
The Discriminator distinguishes between real and fake images. It classifies images as real (from the dataset) or fake (generated by the Generator).
The layers in the discriminator include:
Flatten the image input (28x28x1).
Dense layers with Leaky ReLU activations.
Output layer with a sigmoid activation function, which gives a binary output (real or fake).
Discriminator Architecture:

Input: A 28x28x1 image.
Hidden layers: 512 and 256 units with Leaky ReLU activations.
Output layer: Single unit with sigmoid activation.
4. Compiling the Models:
The Discriminator is compiled with binary cross-entropy loss and the Adam optimizer.
The Generator is compiled indirectly through the combined model.
The combined model (GAN) consists of both the Generator and Discriminator:

The Generator generates images from random noise.
The Discriminator classifies these images as real or fake.
The Discriminator is frozen during the training of the Generator, meaning it won't be updated during this phase.
5. Training the GAN:
The training function train_gan runs the training loop for a given number of epochs.
Discriminator Training:
A batch of real images is selected from the dataset.
A batch of fake images is generated by the Generator using random noise.
The Discriminator is trained on both real and fake images, updating its weights to distinguish between them.
Generator Training:
The Generator is trained to produce images that the Discriminator classifies as real.
The Discriminator is kept frozen during the Generatorâ€™s training.
The train_on_batch method is used to train both models one batch at a time.
The training loop prints the losses of both the Discriminator (D loss) and the Generator (G loss) at each epoch, providing feedback on the progress of training.

6. Saving Generated Images:
Every save_interval epochs, the save_images function is called to generate and display images.
Random noise is passed through the Generator to generate new images, which are rescaled to the range [0, 1] for visualization.
These images are displayed using matplotlib in a grid of examples generated images.
7. Training the GAN:
The GAN is trained for 10,000 epochs, with a batch size of 64. Every 1,000 epochs, generated images are saved and displayed







import os
import zipfile
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Flatten, Reshape, LeakyReLU, BatchNormalization, Conv2DTranspose, Conv2D

# Unzip the uploaded file
file_path = 'fashion-mnist_test.csv.zip'
unzipped_path = 'fashion-mnist_test[1].csv'

with zipfile.ZipFile(file_path, 'r') as zip_ref:
    zip_ref.extractall('/mnt/data/')

# Load the Fashion MNIST dataset
fashion_mnist = pd.read_csv(unzipped_path)

# Separate labels from pixel data (assuming first column is the label)
X_train = fashion_mnist.iloc[:, 1:].values  # Drop the first column (labels)
X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0  # Normalize to [0, 1]

print(f"Shape of X_train: {X_train.shape}")  # Print to verify the shape

# Define the Generator model
def build_generator():
    model = Sequential()
    model.add(Dense(256, input_dim=100))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Dense(512))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Dense(1024))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Dense(28 * 28 * 1, activation='tanh'))  # The output size is 28x28x1
    model.add(Reshape((28, 28, 1)))
    return model

# Define the Discriminator model
def build_discriminator():
    model = Sequential()
    model.add(Flatten(input_shape=(28, 28, 1)))
    model.add(Dense(512))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dense(256))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dense(1, activation='sigmoid'))  # Binary classification (Real/Fake)
    return model

# Compile the Discriminator
discriminator = build_discriminator()
discriminator.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0002, 0.5), metrics=['accuracy'])

generator = build_generator()

# Create the combined GAN model (Generator + Discriminator)
z = tf.keras.Input(shape=(100,))  # Random noise input to the generator
fake_image = generator(z)
discriminator.trainable = False  # Freeze the discriminator during generator training
validity = discriminator(fake_image)
combined = Model(z, validity)
combined.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0002, 0.5))

# Training the GAN
def train_gan(epochs, batch_size=64, save_interval=100):
    
    valid = np.ones((batch_size, 1))
    fake = np.zeros((batch_size, 1))
    
    for epoch in range(epochs):
        
        # Train the Discriminator
        idx = np.random.randint(0, X_train.shape[0], batch_size)
        real_images = X_train[idx]
        
        noise = np.random.normal(0, 1, (batch_size, 100))  # Random noise for the generator
        generated_images = generator.predict(noise)
        
        d_loss_real = discriminator.train_on_batch(real_images, valid)
        d_loss_fake = discriminator.train_on_batch(generated_images, fake)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
        
        # Train the Generator
        noise = np.random.normal(0, 1, (batch_size, 100))
        g_loss = combined.train_on_batch(noise, valid)
        
        # Print the progress
        print(f"Epoch {epoch+1}/{epochs} [D loss: {d_loss[0]:.4f}, acc.: {100 * d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]")
        
        # Save generated images every 'save_interval' epochs
        if epoch % save_interval == 0:
            save_images(epoch)

# Function to save images generated by the Generator
def save_images(epoch, examples=10):
    noise = np.random.normal(0, 1, (examples, 100))
    generated_images = generator.predict(noise)
    
    # Rescale images to [0, 1] for visualization
    generated_images = 0.5 * generated_images + 0.5  
    fig, axs = plt.subplots(1, examples, figsize=(20, 4))
    for i in range(examples):
        axs[i].imshow(generated_images[i, :, :, 0], cmap='gray')
        axs[i].axis('off')
    plt.show()

# Train the GAN
epochs = 10000  # Number of epochs to train the GAN
batch_size = 64  # Batch size
save_interval = 1000  # Save interval for generating and saving images

# Call the training function
train_gan(epochs, batch_size, save_interval)  # Training the GAN model
